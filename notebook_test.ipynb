{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e41422f",
   "metadata": {},
   "source": [
    "Dataset: LIVECell (microscopy, instance segmentation)\n",
    "Model: SAM ViT-B\n",
    "Training scope:\n",
    "\n",
    "Image encoder: frozen\n",
    "\n",
    "Prompt encoder: frozen\n",
    "\n",
    "Mask decoder: trainable\n",
    "\n",
    "This mirrors how LLMs are adapted in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42dfd635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy<2.3.0,>=2 (from opencv-python)\n",
      "  Using cached numpy-2.2.6-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Using cached opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl (39.0 MB)\n",
      "Using cached numpy-2.2.6-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Installing collected packages: numpy, opencv-python\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.5\n",
      "    Uninstalling numpy-2.3.5:\n",
      "      Successfully uninstalled numpy-2.3.5\n",
      "Successfully installed numpy-2.2.6 opencv-python-4.12.0.88\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python\n",
    "# This magic install is the last resort due to mixmatch of venv in bash and notebook kernel, no choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b5e090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.11-cp312-abi3-win_amd64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy in d:\\projects\\.venv\\lib\\site-packages (from pycocotools) (2.2.6)\n",
      "Downloading pycocotools-2.0.11-cp312-abi3-win_amd64.whl (77 kB)\n",
      "Installing collected packages: pycocotools\n",
      "Successfully installed pycocotools-2.0.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fe7848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from pycocotools.coco import COCO\n",
    "import random\n",
    "\n",
    "class LiveCellDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_file, image_size=512):\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.ids = list(self.coco.imgs.keys())\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "\n",
    "        img = cv2.imread(f\"{self.img_dir}/{img_info['file_name']}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "        ann = random.choice(anns)\n",
    "        mask = self.coco.annToMask(ann)\n",
    "\n",
    "        img = cv2.resize(img, (self.image_size, self.image_size))\n",
    "        mask = cv2.resize(mask, (self.image_size, self.image_size),\n",
    "                          interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # generate point prompt inside object\n",
    "        ys, xs = np.where(mask > 0)\n",
    "        idx = random.randint(0, len(xs) - 1)\n",
    "        point = np.array([[xs[idx], ys[idx]]])\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(img).permute(2, 0, 1).float() / 255.0,\n",
    "            torch.from_numpy(mask).unsqueeze(0).float(),\n",
    "            torch.from_numpy(point).float()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "368aac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dice_loss(pred, target, eps=1e-6):\n",
    "    num = 2 * (pred * target).sum()\n",
    "    den = pred.sum() + target.sum() + eps\n",
    "    return 1 - num / den\n",
    "\n",
    "def seg_loss(pred, target):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    return dice_loss(pred, target) + F.binary_cross_entropy(pred, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060c6d85",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'segment_anything'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msegment_anything\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sam_model_registry\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'segment_anything'"
     ]
    }
   ],
   "source": [
    "from segment_anything import sam_model_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a823ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from segment_anything import sam_model_registry\n",
    "# from dataset import LiveCellDataset\n",
    "# from losses import seg_loss\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[\"vit_b\"](checkpoint=\"sam/sam_vit_b_01ec64.pth\")\n",
    "sam.to(device)\n",
    "\n",
    "# freeze everything except mask decoder\n",
    "for p in sam.image_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in sam.prompt_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    sam.mask_decoder.parameters(), lr=1e-4\n",
    ")\n",
    "\n",
    "dataset = LiveCellDataset(\n",
    "    img_dir=\"data/livecell/images/train\",\n",
    "    ann_file=\"data/livecell/annotations/train.json\"\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "sam.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    pbar = tqdm(loader)\n",
    "    for img, mask, point in pbar:\n",
    "        img = img.to(device)\n",
    "        mask = mask.to(device)\n",
    "        point = point.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_embedding = sam.image_encoder(img)\n",
    "\n",
    "        sparse, dense = sam.prompt_encoder(\n",
    "            points=(point, torch.ones(len(point), 1).to(device)),\n",
    "            boxes=None,\n",
    "            masks=None\n",
    "        )\n",
    "\n",
    "        low_res_masks, _ = sam.mask_decoder(\n",
    "            image_embedding,\n",
    "            sam.prompt_encoder.get_dense_pe(),\n",
    "            sparse,\n",
    "            dense,\n",
    "            multimask_output=False\n",
    "        )\n",
    "\n",
    "        loss = seg_loss(low_res_masks, mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description(f\"epoch {epoch} | loss {loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

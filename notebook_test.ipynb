{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e41422f",
   "metadata": {},
   "source": [
    "Dataset: LIVECell (microscopy, instance segmentation)\n",
    "Model: SAM ViT-B\n",
    "Training scope:\n",
    "\n",
    "Image encoder: frozen\n",
    "\n",
    "Prompt encoder: frozen\n",
    "\n",
    "Mask decoder: trainable\n",
    "\n",
    "This mirrors how LLMs are adapted in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42dfd635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install opencv-python\n",
    "#%pip install pycocotools\n",
    "# This magic install is the last resort due to mixmatch of venv in bash and notebook kernel, no choice\n",
    "# this issue is fixed by force install into the absolute path\n",
    "# use bash not cmd, not ps, just bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe7848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2 # so confusing, pip install opencv but module name is cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from pycocotools.coco import COCO # install dataset\n",
    "import random\n",
    "\n",
    "class LiveCellDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_file, image_size=512):\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.ids = list(self.coco.imgs.keys())\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "\n",
    "        img = cv2.imread(f\"{self.img_dir}/{img_info['file_name']}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "        ann = random.choice(anns)\n",
    "        mask = self.coco.annToMask(ann)\n",
    "\n",
    "        img = cv2.resize(img, (self.image_size, self.image_size))\n",
    "        mask = cv2.resize(mask, (self.image_size, self.image_size),\n",
    "                          interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # generate point prompt inside object\n",
    "        ys, xs = np.where(mask > 0)\n",
    "        idx = random.randint(0, len(xs) - 1)\n",
    "        point = np.array([[xs[idx], ys[idx]]])\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(img).permute(2, 0, 1).float() / 255.0,\n",
    "            torch.from_numpy(mask).unsqueeze(0).float(),\n",
    "            torch.from_numpy(point).float()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368aac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dice_loss(pred, target, eps=1e-6):\n",
    "    num = 2 * (pred * target).sum()\n",
    "    den = pred.sum() + target.sum() + eps\n",
    "    return 1 - num / den\n",
    "\n",
    "def seg_loss(pred, target):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    return dice_loss(pred, target) + F.binary_cross_entropy(pred, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22a823ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=5.50s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/814 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (64) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m point = point.to(device)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     image_embedding = \u001b[43msam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m sparse, dense = sam.prompt_encoder(\n\u001b[32m     43\u001b[39m     points=(point, torch.ones(\u001b[38;5;28mlen\u001b[39m(point), \u001b[32m1\u001b[39m).to(device)),\n\u001b[32m     44\u001b[39m     boxes=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     45\u001b[39m     masks=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     46\u001b[39m )\n\u001b[32m     48\u001b[39m low_res_masks, _ = sam.mask_decoder(\n\u001b[32m     49\u001b[39m     image_embedding,\n\u001b[32m     50\u001b[39m     sam.prompt_encoder.get_dense_pe(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m     multimask_output=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     54\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1778\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1789\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1784\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1787\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1788\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1792\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\.venv\\Lib\\site-packages\\segment_anything\\modeling\\image_encoder.py:109\u001b[39m, in \u001b[36mImageEncoderViT.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    107\u001b[39m x = \u001b[38;5;28mself\u001b[39m.patch_embed(x)\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     x = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_embed\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m    112\u001b[39m     x = blk(x)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (32) must match the size of tensor b (64) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from segment_anything import sam_model_registry\n",
    "from dataset import LiveCellDataset\n",
    "from losses import seg_loss\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[\"vit_b\"](checkpoint=\"sam/sam_vit_b_01ec64.pth\")\n",
    "sam.to(device)\n",
    "\n",
    "# freeze everything except mask decoder\n",
    "for p in sam.image_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in sam.prompt_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    sam.mask_decoder.parameters(), lr=1e-4\n",
    ")\n",
    "\n",
    "dataset = LiveCellDataset(\n",
    "    img_dir=\"data/livecell/images/train\",\n",
    "    ann_file=\"data/livecell/annotations/train.json\"\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "sam.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    pbar = tqdm(loader)\n",
    "    for img, mask, point in pbar:\n",
    "        img = img.to(device)\n",
    "        mask = mask.to(device)\n",
    "        point = point.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_embedding = sam.image_encoder(img)\n",
    "\n",
    "        sparse, dense = sam.prompt_encoder(\n",
    "            points=(point, torch.ones(len(point), 1).to(device)),\n",
    "            boxes=None,\n",
    "            masks=None\n",
    "        )\n",
    "\n",
    "        low_res_masks, _ = sam.mask_decoder(\n",
    "            image_embedding,\n",
    "            sam.prompt_encoder.get_dense_pe(),\n",
    "            sparse,\n",
    "            dense,\n",
    "            multimask_output=False\n",
    "        )\n",
    "\n",
    "        loss = seg_loss(low_res_masks, mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description(f\"epoch {epoch} | loss {loss.item():.4f}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ea739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

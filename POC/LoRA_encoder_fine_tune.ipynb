{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d3d241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA on image encoder is compute-heavy (still runs full ViT forward). It saves trainable params, not forward cost.\n",
    "\n",
    "# If your domain gap is mostly “mask style,” decoder-only fine-tune is often stronger per compute than LoRA. LoRA shines when you need to shift representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d142f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LiveCellSAMDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_file, sam_model):\n",
    "        self.img_dir = img_dir\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "\n",
    "        self.transform = ResizeLongestSide(1024)\n",
    "        self.pixel_mean = sam_model.pixel_mean\n",
    "        self.pixel_std = sam_model.pixel_std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        img_info = self.coco.imgs[image_id]\n",
    "\n",
    "        # ---------- load image ----------\n",
    "        image = Image.open(\n",
    "            f\"{self.img_dir}/{img_info['file_name']}\"\n",
    "        ).convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "        H, W = image.shape[:2]\n",
    "\n",
    "        # ---------- load annotations ----------\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # pick ONE instance\n",
    "        ann = anns[np.random.randint(len(anns))]\n",
    "\n",
    "        gt_mask = self.coco.annToMask(ann)      # [H,W]\n",
    "        x, y, w, h = ann[\"bbox\"]\n",
    "        box = np.array([[x, y, x + w, y + h]])  # [1,4]\n",
    "\n",
    "        # ---------- resize ----------\n",
    "        image = self.transform.apply_image(image)\n",
    "        gt_mask = self.transform.apply_image(gt_mask)\n",
    "        box = self.transform.apply_boxes(box, (H, W))\n",
    "\n",
    "        # ---------- pad to 1024 ----------\n",
    "        new_h, new_w = image.shape[:2]\n",
    "        pad_h = 1024 - new_h\n",
    "        pad_w = 1024 - new_w\n",
    "\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "        gt_mask = torch.from_numpy(gt_mask).unsqueeze(0).float()\n",
    "        box = torch.from_numpy(box).float()\n",
    "\n",
    "        image = F.pad(image, (0, pad_w, 0, pad_h))\n",
    "        gt_mask = F.pad(gt_mask, (0, pad_w, 0, pad_h))\n",
    "\n",
    "        # ---------- normalize ----------\n",
    "        #image = (image - self.pixel_mean) / self.pixel_std\n",
    "\n",
    "        return {\n",
    "            \"image\": image,          # [3,1024,1024]\n",
    "            \"mask\": gt_mask,         # [1,1024,1024]\n",
    "            \"box\": box,              # [1,4]\n",
    "            \"image_id\": image_id,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9c4cdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=5.64s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from segment_anything import sam_model_registry\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[\"vit_b\"](checkpoint=\"../sam/sam_vit_b_01ec64.pth\")\n",
    "sam = sam.to(device)\n",
    "\n",
    "dataset = LiveCellSAMDataset(\n",
    "    img_dir=\"../data/livecell/images/train\",\n",
    "    ann_file=\"../data/livecell/annotations/train.json\",\n",
    "    sam_model=sam\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f8e0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    sam.mask_decoder.parameters(), lr=1e-5\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True) # very useful not just for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350e1f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54e08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67e57011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LoRA trainable replacement layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89a69235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps a nn.Linear with a low-rank update:\n",
    "      y = xW^T + b + (alpha/r) * x (BA)^T\n",
    "    where A: (r, in), B: (out, r)\n",
    "\n",
    "    you can patch a transformer layer without rewriting the model. This is the core “adapter” idea: wrap instead of rewrite.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, base: nn.Linear, r: int = 8, alpha: int = 16, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert isinstance(base, nn.Linear) # will wrap linear layer qkv layer\n",
    "        self.base = base\n",
    "        self.r = r # LoRA parameter\n",
    "        self.alpha = alpha # LoRA parameter\n",
    "        self.scale = alpha / r if r > 0 else 0.0\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        # name format consistent with Torch\n",
    "        in_f = base.in_features\n",
    "        out_f = base.out_features\n",
    "\n",
    "        # Low-rank factors\n",
    "        self.A = nn.Parameter(torch.zeros(r, in_f))\n",
    "        self.B = nn.Parameter(torch.zeros(out_f, r))\n",
    "        # because LoRALinear is a module object in Torch and because we set attribute as Parameter\n",
    "        # A and B will be visible in the compute graph\n",
    "\n",
    "        # Init: A ~ Kaiming, B = 0 makes initial update = 0 (safe)\n",
    "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.B)\n",
    "        # wrap should start with BA = 0 and thus the same linear layer as before\n",
    "\n",
    "        # Freeze base params\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.base(x)\n",
    "        # x: (..., in_f)\n",
    "        # (x @ A^T): (..., r)\n",
    "        # (..., r) @ B^T: (..., out_f)\n",
    "        lora = (self.dropout(x) @ self.A.t()) @ self.B.t()\n",
    "        return y + self.scale * lora\n",
    "\n",
    "\n",
    "def mark_only_lora_trainable(model: nn.Module):\n",
    "    for n, p in model.named_parameters():\n",
    "        p.requires_grad = (\"A\" in n or \"B\" in n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d4dee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement encoder layer replacment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dc0d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#from lora import LoRALinear\n",
    "\n",
    "\n",
    "def _replace_module(parent: nn.Module, child_name: str, new_module: nn.Module):\n",
    "    setattr(parent, child_name, new_module)\n",
    "\n",
    "# Why name-based? Because you want to patch specific logical roles:\n",
    "# qkv: attention query/key/value projection (high leverage)\n",
    "# proj: attention output projection (also high leverage)\n",
    "\n",
    "def apply_lora_to_sam_image_encoder(\n",
    "    sam_model: nn.Module,\n",
    "    r: int = 8,\n",
    "    alpha: int = 16,\n",
    "    dropout: float = 0.0,\n",
    "    target_keywords=(\"qkv\", \"proj\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    Safely replaces selected nn.Linear layers under sam_model.image_encoder with LoRALinear.\n",
    "    Two-pass approach avoids recursion issues caused by modifying modules during traversal.\n",
    "    \"\"\"\n",
    "    # 1) Collect candidates first (no mutation here)\n",
    "    to_replace = []\n",
    "    for parent_name, parent in sam_model.image_encoder.named_modules():\n",
    "        # parent_name is a dotted path-like string (e.g., \"blocks.3.attn\")\n",
    "        # parent is the module object at that path\n",
    "        for child_name, child in parent.named_children():\n",
    "            # You want to identify the exact nn.Linear objects that are direct attributes of parent.\n",
    "            # Replacement must happen at that attribute boundary.\n",
    "            if isinstance(child, nn.Linear):\n",
    "                full_name = f\"{parent_name}.{child_name}\".strip(\".\")\n",
    "                # This creates a human-readable identifier like:\n",
    "                # blocks.5.attn.qkv\n",
    "                # blocks.5.attn.proj\n",
    "                if any(k in full_name for k in target_keywords):\n",
    "                    to_replace.append((parent, child_name, full_name, child))\n",
    "                    # parent: the object to mutate\n",
    "                    # child_name: which attribute to set on that parent\n",
    "                    # full_name: logging only\n",
    "                    # child: the original linear module (becomes base in LoRALinear)\n",
    "\n",
    "    # 2) Replace in a second pass\n",
    "    replaced_names = []\n",
    "    for parent, child_name, full_name, child in to_replace:\n",
    "        setattr(parent, child_name, LoRALinear(child, r=r, alpha=alpha, dropout=dropout))\n",
    "        replaced_names.append(full_name)\n",
    "\n",
    "    return replaced_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff96ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the training into a function, it is OK as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bcaf650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from segment_anything import sam_model_registry\n",
    "# from sam_lora_patch import apply_lora_to_sam_image_encoder\n",
    "# from lora import mark_only_lora_trainable\n",
    "\n",
    "\n",
    "def dice_loss(logits, targets, eps=1e-6):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    probs = probs.flatten(1)\n",
    "    targets = targets.flatten(1)\n",
    "    inter = (probs * targets).sum(dim=1)\n",
    "    union = probs.sum(dim=1) + targets.sum(dim=1)\n",
    "    dice = (2 * inter + eps) / (union + eps)\n",
    "    return 1 - dice.mean()\n",
    "\n",
    "\n",
    "def train_lora_sam(\n",
    "    base_ckpt: str,\n",
    "    model_type: str,\n",
    "    dataloader,\n",
    "    device=\"cuda\",\n",
    "    r=8,\n",
    "    alpha=16,\n",
    "    dropout=0.0,\n",
    "    lr=1e-4,\n",
    "    epochs=1,\n",
    "):\n",
    "    sam = sam_model_registry[model_type](checkpoint=base_ckpt)\n",
    "\n",
    "    # Freeze everything first\n",
    "    for p in sam.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Apply LoRA to image encoder (qkv/proj)\n",
    "    replaced = apply_lora_to_sam_image_encoder(sam, r=r, alpha=alpha, dropout=dropout)\n",
    "    print(f\"LoRA injected into {len(replaced)} Linear layers\")\n",
    "\n",
    "    sam.to(device)\n",
    "    sam.train()\n",
    "\n",
    "    # Ensure only LoRA params train\n",
    "    mark_only_lora_trainable(sam)\n",
    "\n",
    "    optim = torch.optim.AdamW([p for p in sam.parameters() if p.requires_grad], lr=lr)\n",
    "    #Optimizer state is allocated only for trainable params, which is the main memory savings of LoRA.\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            # Expect your batch provides:\n",
    "            # image: (B,3,1024,1024) float32 normalized as your SAM pipeline expects\n",
    "            # box: (B,4) in xyxy, 1024-space\n",
    "            # gt_mask: (B,1,1024,1024) {0,1}\n",
    "            image = batch[\"image\"].to(device)\n",
    "            box = batch[\"box\"].to(device)\n",
    "            gt = batch[\"mask\"].to(device) # this is actuall gt_mask, just naming difference\n",
    "\n",
    "            # ---- SAM forward (core idea) ----\n",
    "            # 1) image embeddings from image encoder\n",
    "            image_embeddings = sam.image_encoder(image)\n",
    "\n",
    "            # 2) prompt encoder with boxes\n",
    "            sparse_embeddings, dense_embeddings = sam.prompt_encoder(\n",
    "                points=None,\n",
    "                boxes=box,\n",
    "                masks=None,\n",
    "            )\n",
    "\n",
    "            # 3) mask decoder → low-res logits\n",
    "            low_res_masks, iou_preds = sam.mask_decoder(\n",
    "                image_embeddings=image_embeddings,\n",
    "                image_pe=sam.prompt_encoder.get_dense_pe(), #image_pe is positional encoding for the dense grid.\n",
    "                sparse_prompt_embeddings=sparse_embeddings,\n",
    "                dense_prompt_embeddings=dense_embeddings,\n",
    "                multimask_output=False,\n",
    "            )\n",
    "\n",
    "            # 4) Upsample logits to full 1024 for loss\n",
    "            logits = F.interpolate(low_res_masks, size=gt.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "            # ---- Loss ----\n",
    "            bce = F.binary_cross_entropy_with_logits(logits, gt)\n",
    "            dloss = dice_loss(logits, gt)\n",
    "            loss = bce + dloss\n",
    "            \n",
    "            # BCE handles local correctness\n",
    "            # Dice handles overlap / class imbalance\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            if step % 100 == 0:\n",
    "                print(f\"step {step}, loss {loss.item():.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | loss={loss.item():.4f}\")\n",
    "\n",
    "    return sam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be60bc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA injected into 24 Linear layers\n",
      "step 0, loss 0.4252\n",
      "step 100, loss 0.3073\n",
      "step 200, loss 0.7332\n",
      "step 300, loss 0.2148\n",
      "step 400, loss 0.1726\n",
      "step 500, loss 0.3024\n",
      "step 600, loss 0.3768\n",
      "step 700, loss 0.1789\n",
      "step 800, loss 0.1493\n",
      "step 900, loss 0.1052\n",
      "step 1000, loss 0.1245\n",
      "step 1100, loss 0.2540\n",
      "step 1200, loss 0.0760\n",
      "step 1300, loss 0.1631\n",
      "step 1400, loss 0.4814\n",
      "step 1500, loss 0.1520\n",
      "step 1600, loss 0.0587\n",
      "step 1700, loss 0.1204\n",
      "step 1800, loss 0.1397\n",
      "step 1900, loss 0.1150\n",
      "step 2000, loss 0.2016\n",
      "step 2100, loss 0.2310\n",
      "step 2200, loss 0.0810\n",
      "step 2300, loss 0.1263\n",
      "step 2400, loss 0.0411\n",
      "step 2500, loss 0.1364\n",
      "step 2600, loss 0.1685\n",
      "step 2700, loss 0.4014\n",
      "step 2800, loss 0.0856\n",
      "step 2900, loss 0.0611\n",
      "step 3000, loss 0.0627\n",
      "step 3100, loss 0.0628\n",
      "step 3200, loss 0.1153\n",
      "Epoch 1/1 | loss=0.1826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): LoRALinear(\n",
       "            (base): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (dropout): Identity()\n",
       "          )\n",
       "          (proj): LoRALinear(\n",
       "            (base): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Identity()\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lora_sam(\n",
    "    \"../sam/sam_vit_b_01ec64.pth\",\n",
    "    \"vit_b\",\n",
    "    loader,\n",
    "    device=\"cuda\",\n",
    "    r=8,\n",
    "    alpha=16,\n",
    "    dropout=0.0,\n",
    "    lr=1e-5,\n",
    "    epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cd5a492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def save_lora_weights(sam_model, path=\"lora_image_encoder.pt\"):\n",
    "    lora_state = {}\n",
    "    for name, param in sam_model.named_parameters():\n",
    "        # LoRA params are named like \"...A\" and \"...B\" in our module\n",
    "        if name.endswith(\".A\") or name.endswith(\".B\"):\n",
    "            lora_state[name] = param.detach().cpu()\n",
    "    torch.save(lora_state, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07ebc93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from segment_anything import sam_model_registry\n",
    "#from sam_lora_patch import apply_lora_to_sam_image_encoder\n",
    "\n",
    "def load_sam_with_lora(base_ckpt, model_type, lora_path, device=\"cuda\", r=8, alpha=16, dropout=0.0):\n",
    "    sam = sam_model_registry[model_type](checkpoint=base_ckpt)\n",
    "    apply_lora_to_sam_image_encoder(sam, r=r, alpha=alpha, dropout=dropout)\n",
    "\n",
    "    lora_state = torch.load(lora_path, map_location=\"cpu\")\n",
    "    missing, unexpected = sam.load_state_dict(lora_state, strict=False)\n",
    "    # missing is fine (we are only loading LoRA params)\n",
    "    sam.to(device)\n",
    "    sam.eval()\n",
    "    return sam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab407f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
